{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sksurv.datasets import load_flchain, load_gbsg2\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis, CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.preprocessing import OneHotEncoder, encode_categorical\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import scipy.optimize as opt\n",
    "\n",
    "set_config(display=\"text\")  # displays text representation of estimators\n",
    "plt.rcParams[\"figure.figsize\"] = [7.2, 4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_marker(n_samples, m, hazard_ratio, baseline_hazard, rnd):\n",
    "    # create synthetic risk score\n",
    "    X = np.array(rnd.randn(n_samples, m))\n",
    "    w = np.expand_dims(np.array(rnd.uniform(size=m)), axis=0).flatten() # weights\n",
    "\n",
    "    # create linear model\n",
    "    logits = np.dot(np.dot(X,w.T), np.log(hazard_ratio))\n",
    "\n",
    "    # draw actual survival times from exponential distribution,\n",
    "    # refer to Bender et al. (2005), https://doi.org/10.1002/sim.2059\n",
    "    u = rnd.uniform(size=n_samples)\n",
    "    time_event = -np.log(u) / (baseline_hazard * np.exp(logits))\n",
    "\n",
    "    # compute the actual concordance in the absence of censoring\n",
    "    Xactual = np.squeeze(np.dot(X, w.T))\n",
    "    actual = concordance_index_censored(np.ones(n_samples, dtype=bool), time_event, Xactual)\n",
    "    return X, time_event, actual[0], w # risk scores, time events, actual concordance, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_survival_data(n_samples, m, hazard_ratio, baseline_hazard, percentage_cens, rnd):\n",
    "    X, time_event, actual_c, w = generate_marker(n_samples, m, hazard_ratio, baseline_hazard, rnd)\n",
    "\n",
    "    def get_observed_time(x): # this censors certain time events\n",
    "        rnd_cens = np.random.RandomState(0)\n",
    "        # draw censoring times\n",
    "        time_censor = rnd_cens.uniform(high=x, size=n_samples)\n",
    "        event = time_event < time_censor\n",
    "        time = np.where(event, time_event, time_censor)\n",
    "        return event, time # returns bool array of if event occured or not/censored, and the time it occured/ was censored\n",
    "\n",
    "    def censoring_amount(x): # finds optimal time event censoring will be as close to desired censored amount\n",
    "        event, _ = get_observed_time(x)\n",
    "        cens = 1.0 - event.sum() / event.shape[0]\n",
    "        return (cens - percentage_cens) ** 2\n",
    "\n",
    "    # search for upper limit to obtain the desired censoring amount\n",
    "    res = opt.minimize_scalar(censoring_amount, method=\"bounded\", bounds=(0, time_event.max()))\n",
    "\n",
    "    # compute observed time\n",
    "    event, time = get_observed_time(res.x) # now that we have the optimal time event, we use that to get all the events and times\n",
    "\n",
    "    # upper time limit such that the probability of being censored is non-zero for `t > tau`\n",
    "    # we are finding the latest observed event time, and only keeping those events where time is < tau to decrease biases\n",
    "    tau = time[event].max()\n",
    "    y = Surv.from_arrays(event=event, time=time)\n",
    "    mask = time < tau\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "    return X_test, y_test, y, actual_c, w # risk scores, event/time with tau applied, event/time without tau, actual c, weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(n_samples, m, hazard_ratio, n_repeats=100):\n",
    "    measures = (\n",
    "        \"censoring\",\n",
    "        \"Harrel's C\",\n",
    "        \"Uno's C\",\n",
    "        \"Mean AUC\",\n",
    "    )\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    for measure in measures:\n",
    "        data_mean[measure] = []\n",
    "        data_std[measure] = []\n",
    "\n",
    "    rnd = np.random.RandomState(seed=987)\n",
    "    # iterate over different amount of censoring\n",
    "    for cens in (0.1, 0.25, 0.4, 0.5, 0.6, 0.7):\n",
    "        data = {\n",
    "            \"censoring\": [],\n",
    "            \"Harrel's C\": [],\n",
    "            \"Uno's C\": [],\n",
    "            \"Mean AUC\" : [],\n",
    "        }\n",
    "\n",
    "        # repeaditly perform simulation\n",
    "        for _ in range(n_repeats):\n",
    "            # generate data\n",
    "            X_test, y_test, y_train, actual_c, w = generate_survival_data(\n",
    "                n_samples, m, hazard_ratio, baseline_hazard=0.1, percentage_cens=cens, rnd=rnd\n",
    "            )\n",
    "\n",
    "            rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, \n",
    "                    max_features=\"sqrt\", n_jobs=-1, random_state=rnd)\n",
    "            \n",
    "            rsf.fit(X_test, y_test)\n",
    "\n",
    "            # predict risk scores (lower predicted survival time = higher risk)\n",
    "            risk_scores = -rsf.predict(X_test) # doing neg bec RSF does higher num = better survival time, but harrells c higher num = higher risk or earlier\n",
    "\n",
    "\n",
    "            # random time points to check auc\n",
    "            times = np.linspace(y_train[\"time\"].min(), y_train[\"time\"].max(), 50)\n",
    "\n",
    "            print(\"y train\", y_train[\"time\"].max())\n",
    "            print(\"y test\", y_test[\"time\"].max())\n",
    "            print(\"times\", times.max())\n",
    "\n",
    "            \n",
    "            # Compute cumulative dynamic AUC\n",
    "            # this requires that survival times survival_test lie within the range of survival times survival_train\n",
    "            _, aucs, _ = cumulative_dynamic_auc(y_train, y_test, risk_scores, times)\n",
    "\n",
    "            # Take the mean AUC across all times\n",
    "            mean_auc = np.mean(aucs)\n",
    "\n",
    "            # estimate c-index\n",
    "            c_harrell = concordance_index_censored(y_test[\"event\"], y_test[\"time\"], risk_scores)\n",
    "            c_uno = concordance_index_ipcw(y_train, y_test, risk_scores)\n",
    "            \n",
    "            # save results\n",
    "            data[\"censoring\"].append(100.0 - y_test[\"event\"].sum() * 100.0 / y_test.shape[0])\n",
    "            data[\"Harrel's C\"].append(actual_c - c_harrell[0])\n",
    "            data[\"Uno's C\"].append(actual_c - c_uno[0])\n",
    "            data[\"Mean AUC\"].append(mean_auc)\n",
    "\n",
    "        # aggregate results\n",
    "        for key, values in data.items():\n",
    "            data_mean[key].append(np.mean(data[key]))\n",
    "            data_std[key].append(np.std(data[key], ddof=1))\n",
    "\n",
    "    data_mean = pd.DataFrame.from_dict(data_mean)\n",
    "    data_std = pd.DataFrame.from_dict(data_std)\n",
    "    return data_mean, data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data_mean, data_std, **kwargs):\n",
    "    index = pd.Index(data_mean[\"censoring\"].round(3), name=\"mean percentage censoring\")\n",
    "    for df in (data_mean, data_std):\n",
    "        df.drop(\"censoring\", axis=1, inplace=True)\n",
    "        df.index = index\n",
    "\n",
    "    ax = data_mean.plot.bar(yerr=data_std, **kwargs)\n",
    "    ax.set_ylabel(\"Actual C - Estimated C\")\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.axhline(0.0, color=\"gray\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train 68.64415691006276 (True, 14.97480562)\n",
      "y test 58.62084022942785\n",
      "times 68.64415691006276\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all times must be within follow-up time of test data: [0.0350555015279663; 58.62084022942785[",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_mean, data_std \u001b[38;5;241m=\u001b[39m \u001b[43msimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhazard_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ylim \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.035\u001b[39m, \u001b[38;5;241m0.5\u001b[39m] \n\u001b[1;32m      3\u001b[0m plot_results(data_mean, data_std, ylim\u001b[38;5;241m=\u001b[39mylim, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m, in \u001b[0;36msimulation\u001b[0;34m(n_samples, m, hazard_ratio, n_repeats)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes\u001b[39m\u001b[38;5;124m\"\u001b[39m, times\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Compute cumulative dynamic AUC\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# this requires that survival times survival_test lie within the range of survival times survival_train\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m _, aucs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcumulative_dynamic_auc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrisk_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Take the mean AUC across all times\u001b[39;00m\n\u001b[1;32m     53\u001b[0m mean_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(aucs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sksurv/metrics.py:448\u001b[0m, in \u001b[0;36mcumulative_dynamic_auc\u001b[0;34m(survival_train, survival_test, estimate, times, tied_tol)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"Estimator of cumulative/dynamic AUC for right-censored time-to-event data.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mThe receiver operating characteristic (ROC) curve and the area under the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m       Statistical Methods in Medical Research, 2014.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m test_event, test_time \u001b[38;5;241m=\u001b[39m check_y_survival(survival_test)\n\u001b[0;32m--> 448\u001b[0m estimate, times \u001b[38;5;241m=\u001b[39m \u001b[43m_check_estimate_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcumulative_dynamic_auc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m estimate\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    451\u001b[0m n_times \u001b[38;5;241m=\u001b[39m times\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sksurv/metrics.py:76\u001b[0m, in \u001b[0;36m_check_estimate_2d\u001b[0;34m(estimate, test_time, time_points, estimator)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_estimate_2d\u001b[39m(estimate, test_time, time_points, estimator):\n\u001b[1;32m     75\u001b[0m     estimate \u001b[38;5;241m=\u001b[39m check_array(estimate, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, allow_nd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimate\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m---> 76\u001b[0m     time_points \u001b[38;5;241m=\u001b[39m \u001b[43m_check_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     check_consistent_length(test_time, estimate)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m estimate\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m time_points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sksurv/metrics.py:67\u001b[0m, in \u001b[0;36m_check_times\u001b[0;34m(test_time, times)\u001b[0m\n\u001b[1;32m     64\u001b[0m times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(times)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;129;01mor\u001b[39;00m times\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m test_time\u001b[38;5;241m.\u001b[39mmin():\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall times must be within follow-up time of test data: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_time\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m times\n",
      "\u001b[0;31mValueError\u001b[0m: all times must be within follow-up time of test data: [0.0350555015279663; 58.62084022942785["
     ]
    }
   ],
   "source": [
    "data_mean, data_std = simulation(n_samples=100, m=3, hazard_ratio=2.0)\n",
    "ylim = [-0.035, 0.5] \n",
    "plot_results(data_mean, data_std, ylim=ylim, figsize=(10, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
