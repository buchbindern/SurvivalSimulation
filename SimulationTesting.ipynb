{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sksurv.datasets import load_flchain, load_gbsg2\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis, CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.preprocessing import OneHotEncoder, encode_categorical\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import scipy.optimize as opt\n",
    "\n",
    "set_config(display=\"text\")  # displays text representation of estimators\n",
    "plt.rcParams[\"figure.figsize\"] = [7.2, 4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_marker(n_samples, m, hazard_ratio, baseline_hazard, rnd):\n",
    "    # create synthetic risk score\n",
    "    X = np.array(rnd.randn(n_samples, m))\n",
    "    w = np.expand_dims(np.array(rnd.uniform(size=m)), axis=0).flatten() # weights\n",
    "\n",
    "    # create linear model\n",
    "    logits = np.dot(np.dot(X,w.T), np.log(hazard_ratio))\n",
    "\n",
    "    # draw actual survival times from exponential distribution,\n",
    "    # refer to Bender et al. (2005), https://doi.org/10.1002/sim.2059\n",
    "    u = rnd.uniform(size=n_samples)\n",
    "    time_event = -np.log(u) / (baseline_hazard * np.exp(logits))\n",
    "\n",
    "    # compute the actual concordance in the absence of censoring\n",
    "    Xactual = np.squeeze(np.dot(X, w.T))\n",
    "    actual = concordance_index_censored(np.ones(n_samples, dtype=bool), time_event, Xactual)\n",
    "    return X, time_event, actual[0], w # risk scores, time events, actual concordance, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_survival_data(n_samples, m, hazard_ratio, baseline_hazard, percentage_cens, rnd):\n",
    "    X, time_event, actual_c, w = generate_marker(n_samples, m, hazard_ratio, baseline_hazard, rnd)\n",
    "\n",
    "    def get_observed_time(x): # this censors certain time events\n",
    "        rnd_cens = np.random.RandomState(0)\n",
    "        # draw censoring times\n",
    "        time_censor = rnd_cens.uniform(high=x, size=n_samples)\n",
    "        event = time_event < time_censor\n",
    "        time = np.where(event, time_event, time_censor)\n",
    "        return event, time # returns bool array of if event occured or not/censored, and the time it occured/ was censored\n",
    "\n",
    "    def censoring_amount(x): # finds optimal time event censoring will be as close to desired censored amount\n",
    "        event, _ = get_observed_time(x)\n",
    "        cens = 1.0 - event.sum() / event.shape[0]\n",
    "        return (cens - percentage_cens) ** 2\n",
    "\n",
    "    # search for upper limit to obtain the desired censoring amount\n",
    "    res = opt.minimize_scalar(censoring_amount, method=\"bounded\", bounds=(0, time_event.max()))\n",
    "\n",
    "    # compute observed time\n",
    "    event, time = get_observed_time(res.x) # now that we have the optimal time event, we use that to get all the events and times\n",
    "\n",
    "    # upper time limit such that the probability of being censored is non-zero for `t > tau`\n",
    "    # we are finding the latest observed event time, and only keeping those events where time is < tau to decrease biases\n",
    "    tau = time[event].max()\n",
    "    y = Surv.from_arrays(event=event, time=time)\n",
    "    mask = time < tau\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "\n",
    "    return X_test, y_test, y, actual_c, w # risk scores, event/time with tau applied, event/time without tau, actual c, weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(n_samples, m, hazard_ratio, n_repeats=100):\n",
    "    measures = (\n",
    "        \"censoring\",\n",
    "        \"Harrel's C\",\n",
    "        \"Uno's C\",\n",
    "        \"AUC\",\n",
    "        \"Brier\",\n",
    "    )\n",
    "    data_mean = {}\n",
    "    data_std = {}\n",
    "    for measure in measures:\n",
    "        data_mean[measure] = []\n",
    "        data_std[measure] = []\n",
    "\n",
    "    rnd = np.random.RandomState(seed=987)\n",
    "    # iterate over different amount of censoring\n",
    "    for cens in (0.1, 0.25, 0.4, 0.5, 0.6, 0.7):\n",
    "        data = {\n",
    "            \"censoring\": [],\n",
    "            \"Harrel's C\": [],\n",
    "            \"Uno's C\": [],\n",
    "            \"AUC\" : [],\n",
    "            \"Brier\": [],\n",
    "        }\n",
    "\n",
    "        # repeaditly perform simulation\n",
    "        for _ in range(n_repeats):\n",
    "            # generate data\n",
    "            X_test, y_test, y_train, actual_c, w = generate_survival_data(\n",
    "                n_samples, m, hazard_ratio, baseline_hazard=0.1, percentage_cens=cens, rnd=rnd\n",
    "            )\n",
    "\n",
    "            rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, \n",
    "                    max_features=\"sqrt\", n_jobs=-1, random_state=rnd)\n",
    "            \n",
    "            rsf.fit(X_test, y_test)\n",
    "\n",
    "            # predict risk scores (lower predicted survival time = higher risk)\n",
    "            risk_scores = -rsf.predict(X_test) # doing neg bec RSF does higher num = better survival time, but harrells c higher num = higher risk or earlier\n",
    "\n",
    "\n",
    "            # random time points to check auc\n",
    "            times = np.linspace(y_test[\"time\"].min()+0.1, y_test[\"time\"].max()-0.1, 10)\n",
    "            \n",
    "            # Compute cumulative dynamic AUC\n",
    "            # this requires that survival times survival_test lie within the range of survival times survival_train\n",
    "            aucs, _ = cumulative_dynamic_auc(y_train, y_test, risk_scores, times)\n",
    "            mean_auc = np.nanmean(aucs) # manually get mean to takes out NaNs\n",
    "\n",
    "            # Breier\n",
    "            # Estimated probability of remaining event-free at time points specified by times\n",
    "            pred_func = rsf.predict_survival_function(X_test) # the predict survival fundtion  \n",
    "            preds = np.asarray([[fn(t) for t in times] for fn in pred_func]) # the actual points on the function\n",
    "\n",
    "            brier = integrated_brier_score(y_train, y_test, preds, times)\n",
    "\n",
    "            # estimate c-index\n",
    "            c_harrell = concordance_index_censored(y_test[\"event\"], y_test[\"time\"], risk_scores)\n",
    "            c_uno = concordance_index_ipcw(y_train, y_test, risk_scores)\n",
    "\n",
    "            \n",
    "            # save results\n",
    "            data[\"censoring\"].append(100.0 - y_test[\"event\"].sum() * 100.0 / y_test.shape[0])\n",
    "            data[\"Harrel's C\"].append(actual_c - c_harrell[0])\n",
    "            data[\"Uno's C\"].append(actual_c - c_uno[0])\n",
    "            data[\"AUC\"].append(actual_c- mean_auc)\n",
    "            data[\"Brier\"].append(actual_c - brier)\n",
    "            \n",
    "            \n",
    "        # aggregate results\n",
    "        for key, values in data.items():\n",
    "            data_mean[key].append(np.mean(data[key]))\n",
    "            data_std[key].append(np.std(data[key], ddof=1))\n",
    "\n",
    "    data_mean = pd.DataFrame.from_dict(data_mean)\n",
    "    data_std = pd.DataFrame.from_dict(data_std)\n",
    "    return data_mean, data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data_mean, data_std, **kwargs):\n",
    "    index = pd.Index(data_mean[\"censoring\"].round(3), name=\"mean percentage censoring\")\n",
    "    for df in (data_mean, data_std):\n",
    "        df.drop(\"censoring\", axis=1, inplace=True)\n",
    "        df.index = index\n",
    "\n",
    "    ax = data_mean.plot.bar(yerr=data_std, **kwargs)\n",
    "    ax.set_ylabel(\"Actual C - Estimated C\")\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.axhline(0.0, color=\"gray\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SurvivalAnalysisMixin._predict_survival_function() missing 2 required positional arguments: 'prediction' and 'return_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_mean, data_std \u001b[38;5;241m=\u001b[39m \u001b[43msimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhazard_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ylim \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.035\u001b[39m, \u001b[38;5;241m0.5\u001b[39m] \n\u001b[1;32m      3\u001b[0m plot_results(data_mean, data_std, ylim\u001b[38;5;241m=\u001b[39mylim, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m, in \u001b[0;36msimulation\u001b[0;34m(n_samples, m, hazard_ratio, n_repeats)\u001b[0m\n\u001b[1;32m     48\u001b[0m mean_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(aucs) \u001b[38;5;66;03m# manually get mean to takes out NaNs\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Breier\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Estimated probability of remaining event-free at time points specified by times\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m pred_func \u001b[38;5;241m=\u001b[39m \u001b[43mrsf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_survival_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# the predict survival fundtion  \u001b[39;00m\n\u001b[1;32m     53\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([[fn(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m times] \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m pred_func]) \u001b[38;5;66;03m# the actual points on the function\u001b[39;00m\n\u001b[1;32m     55\u001b[0m brier \u001b[38;5;241m=\u001b[39m integrated_brier_score(y_train, y_test, preds, times)\n",
      "\u001b[0;31mTypeError\u001b[0m: SurvivalAnalysisMixin._predict_survival_function() missing 2 required positional arguments: 'prediction' and 'return_array'"
     ]
    }
   ],
   "source": [
    "data_mean, data_std = simulation(n_samples=100, m=3, hazard_ratio=2.0)\n",
    "ylim = [-0.035, 0.5] \n",
    "plot_results(data_mean, data_std, ylim=ylim, figsize=(10, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
